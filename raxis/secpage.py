import requests
import json # To pretty-print JSON response if applicable
from typing import List, Union
from .axis import extract_report_information,transform_data
from datetime import datetime
from stockutils import read_first_line,write_first_line
from stockutils import print_table,insert_into_database
import logging
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)


def fetch_axisdirect_reports(
    ids_list: List[Union[int, str]],
    offset_value: Union[int, str]
) -> str:
    """
    Fetches reports from AxisDirect insights using a POST request,
    mimicking the provided curl command.

    Args:
        ids_list (List[Union[int, str]]): A list of IDs to be included in the 'ids' parameter.
                                          Will be converted to a comma-separated string.
        offset_value (Union[int, str]): The offset value for pagination.

    IMPORTANT:
    1. The 'cookie' header contains session-specific information and will
       likely expire quickly. You might need to get fresh cookies from
       your browser's developer tools if this script fails.
    2. Other parameters like 'token_value' in the 'data' payload are often
       dynamically generated by the website's JavaScript. For reliable
       and comprehensive scraping (e.g., pagination), you might need to
       first visit the 'referer' page and extract these values programmatically.

    Returns:
        str: The text content of the response. Returns None if an error occurs.
    """
    url = 'https://simplehai.axisdirect.in/app/index.php/insights/reports/search'

    # Headers from the curl command
    headers = {
        'accept': '*/*',
        'accept-language': 'en-US,en;q=0.9',
        'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',
        # !!! You MUST replace this with your own, fresh cookies if the request fails !!!
        # These cookies are session-specific and will expire.
        'origin': 'https://simplehai.axisdirect.in',
        'priority': 'u=1, i',
        'referer': 'https://simplehai.axisdirect.in/research/reports/fundamental',
        'sec-ch-ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'x-dtpc': '3$570055674_520h148vATEUAPUBUBPSUTFKCFJBURPCMAWKDMOA-0e0',
        'x-requested-with': 'XMLHttpRequest',
    }

    # Data payload from --data-raw
    # Convert ids_list to a comma-separated string
    ids_str = ','.join(map(str, ids_list))

    data = {
        'token_value': '', # This might need to be dynamic if the site uses it
        'entity_id': '',
        'client_name': '',
        'last_login_time': '',
        'searchStr': '',
        'categoryId': '',
        'date': 'all',
        'format': 'all',
        'dbmodel': 'RsFundamentalReports',
        'offsetVal': str(offset_value), # Ensure offset_value is a string
        'sortVal': '',
        'ids': ids_str,
    }

    
    try:
        response = requests.post(url, headers=headers, data=data)
        logging.info(f"Received HTTP {response.status_code} from {url}")
        
        # Raise error if status is 4xx or 5xx
        response.raise_for_status()
        
        # Determine response type
        content_type = response.headers.get("Content-Type", "").lower()
        logging.debug(f"Response Content-Type: {content_type}")
        
        if "application/json" in content_type:
            try:
                json_content = response.json()
                logging.info(f"JSON response parsed successfully from {url}")
                logging.debug(f"Response JSON: {json.dumps(json_content, indent=2)}")
                return json_content
            except json.JSONDecodeError:
                logging.warning(f"Invalid JSON received from {url}")
                logging.debug(f"Raw Response: {response.text[:1000]}")
                return response.text
        else:
            logging.info(f"Non-JSON response received from {url}")
            logging.debug(f"Response Text: {response.text[:1000]}")
            return response.text
    
    except requests.exceptions.HTTPError as http_err:
        logging.error(f"HTTP error: {http_err}")
    except requests.exceptions.ConnectionError as conn_err:
        logging.error(f"Connection error: {conn_err}")
    except requests.exceptions.Timeout as timeout_err:
        logging.error(f"Timeout error: {timeout_err}")
    except requests.exceptions.RequestException as req_err:
        logging.error(f"Request exception: {req_err}")
    except Exception as e:
        logging.error(f"Unexpected error: {e}")

    return None








def axis_main():
 lastfound=False
 reports=[]
 ids=[]
 ldate_string=read_first_line("./cntrfiles/axis.txt").strip()
 lastdate=datetime.strptime(ldate_string,  "%B %d, %Y") 
 while not lastfound:
  try:
    k=fetch_axisdirect_reports(ids,len(ids)) 
  except Exception as e:
    logging.error("Issue with Json format. Exiting .. . Will add already found reports")
    break
  results,ids=extract_report_information(k,ids)
  reps,lastfound=transform_data(lastdate,results)
  reports.extend(reps)
 print_table(reports,logger)
 insert_into_database(reports,"axis")
 if len(reports) >0:
  write_first_line('./cntrfiles/axis.txt',reports[0]["report-date"] )
 print(reports)
